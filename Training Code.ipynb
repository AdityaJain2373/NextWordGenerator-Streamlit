{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "with open(\"Dataset2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text).lower()\n",
    "words = [word for word in text.split() if len(word) > 2]\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "stoi = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "stoi['<PAD>'] = 0\n",
    "itos = {i: word for word, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "def find_closest_word_vector(unknown_word_vector, model):\n",
    "    word_embeddings = model.emb.weight.detach().cpu().numpy()\n",
    "    similarities = cosine_similarity(unknown_word_vector.reshape(1, -1), word_embeddings)\n",
    "    closest_idx = np.argmax(similarities)\n",
    "    return itos[closest_idx]  \n",
    "\n",
    "def handle_unknown_words(words, model):\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word in stoi:\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            random_vector = np.random.randn(model.emb.embedding_dim)\n",
    "            closest_word = find_closest_word_vector(random_vector, model)\n",
    "            print(f\"Unknown word '{word}' replaced with '{closest_word}'\")\n",
    "            processed_words.append(closest_word)\n",
    "    return processed_words\n",
    "\n",
    "class NextWordPredictor(nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n",
    "        self.activation = activation\n",
    "        self.lin2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x).view(x.size(0), -1)\n",
    "        x = self.activation(self.lin1(x))\n",
    "        return self.lin2(x)\n",
    "\n",
    "def load_dataset(block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(words) - block_size):\n",
    "        context = words[i:i + block_size]\n",
    "        target = words[i + block_size]\n",
    "        X.append([stoi.get(w, 0) for w in context])\n",
    "        Y.append(stoi.get(target, 0))\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return DataLoader(TensorDataset(X, Y), batch_size=128, shuffle=True)\n",
    "\n",
    "def train_model(emb_dim, hidden_size, activation_fn, block_size, random_seed, epochs):\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    model = NextWordPredictor(block_size, vocab_size, emb_dim, hidden_size, activation_fn).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=stoi['<PAD>'])\n",
    "    loader = load_dataset(block_size)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "\n",
    "    model_path = f\"model_{emb_dim}_{hidden_size}_{activation_fn.__name__}_bs{block_size}_rs{random_seed}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "def load_model(emb_dim, hidden_size, activation_fn, block_size, random_seed):\n",
    "    model = NextWordPredictor(block_size, vocab_size, emb_dim, hidden_size, activation_fn).to(device)\n",
    "    model_path = f\"model_{emb_dim}_{hidden_size}_{activation_fn.__name__}_bs{block_size}_rs{random_seed}.pt\"\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def extract_relationships(model):\n",
    "    embeddings = model.emb.weight.detach().cpu().numpy()\n",
    "    word_categories = {'synonyms': [], 'antonyms': [], 'pronouns': []}\n",
    "    pronouns_list = ['he', 'she', 'it', 'they', 'we', 'you', 'me', 'him', 'her', 'us', 'them']\n",
    "\n",
    "    for word, idx in stoi.items():\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if word in pronouns_list:\n",
    "            word_categories['pronouns'].append(word)\n",
    "        else:\n",
    "            for syn in synsets:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() in stoi and lemma.name() != word:\n",
    "                        word_categories['synonyms'].append((word, lemma.name()))\n",
    "                    if lemma.antonyms():\n",
    "                        antonym = lemma.antonyms()[0].name()\n",
    "                        if antonym in stoi:\n",
    "                            word_categories['antonyms'].append((word, antonym))\n",
    "    return word_categories\n",
    "\n",
    "train_model(256, 1024, torch.relu, 15, 1234, 35)\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "# # emb_dim = 128\n",
    "# train_model(128, 512, torch.relu, 10, 42, epochs)\n",
    "# train_model(128, 512, torch.relu, 15, 42, epochs)\n",
    "\n",
    "# train_model(128, 512, torch.tanh, 5, 42, epochs)\n",
    "# train_model(128, 512, torch.tanh, 10, 42, epochs)\n",
    "# train_model(128, 512, torch.tanh, 15, 42, epochs)\n",
    "\n",
    "# train_model(128, 1024, torch.relu, 5, 42, epochs)\n",
    "# train_model(128, 1024, torch.relu, 10, 42, epochs)\n",
    "# train_model(128, 1024, torch.relu, 15, 42, epochs)\n",
    "\n",
    "# train_model(128, 1024, torch.tanh, 5, 42, epochs)\n",
    "# train_model(128, 1024, torch.tanh, 10, 42, epochs)\n",
    "# train_model(128, 1024, torch.tanh, 15, 42, epochs)\n",
    "\n",
    "# emb_dim = 256\n",
    "# train_model(256, 512, torch.relu, 5, 42, epochs)\n",
    "# train_model(256, 512, torch.relu, 10, 42, epochs)\n",
    "# train_model(256, 512, torch.relu, 15, 42, epochs)\n",
    "\n",
    "# train_model(256, 512, torch.tanh, 5, 42, epochs)\n",
    "# train_model(256, 512, torch.tanh, 10, 42, epochs)\n",
    "# train_model(256, 512, torch.tanh, 15, 42, epochs)\n",
    "\n",
    "# train_model(256, 1024, torch.relu, 5, 42, epochs)\n",
    "# train_model(256, 1024, torch.relu, 10, 42, epochs)\n",
    "\n",
    "# train_model(256, 1024, torch.tanh, 5, 42, epochs)\n",
    "# train_model(256, 1024, torch.tanh, 10, 42, epochs)\n",
    "# train_model(256, 1024, torch.tanh, 15, 42, epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit app code saved to streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "streamlit_code = \"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "import os\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "st.write(f\"Using device: {device}\")\n",
    "\n",
    "# Load variables\n",
    "with open('variables.pkl', 'rb') as f:\n",
    "    variables = pickle.load(f)\n",
    "\n",
    "vocab_size = variables['vocab_size']\n",
    "stoi = variables['stoi']\n",
    "itos = variables['itos']\n",
    "default_block_size = variables['block_size']\n",
    "default_random_seed = variables['random_seed']\n",
    "\n",
    "class NextWordPredictor(nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n",
    "        self.activation = activation\n",
    "        self.lin2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x).view(x.size(0), -1)\n",
    "        x = self.activation(self.lin1(x))\n",
    "        return self.lin2(x)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(emb_dim, hidden_size, activation, block_size, random_seed):\n",
    "    activation_fn = {\"Tanh\": torch.tanh, \"ReLU\": torch.relu, \"Sigmoid\": torch.sigmoid}[activation]\n",
    "    model_path = os.path.join(\"models\", f\"model_{emb_dim}_{hidden_size}_{activation}_bs{block_size}_rs{random_seed}.pt\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        st.error(f\"Model {model_path} not found. Make sure the model file exists.\")\n",
    "        st.stop()\n",
    "\n",
    "    model = NextWordPredictor(block_size, vocab_size, emb_dim, hidden_size, activation_fn).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def find_closest_word(embedding, model):\n",
    "    word_embeddings = model.emb.weight.detach().cpu().numpy()\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), word_embeddings)\n",
    "    closest_idx = np.argmax(similarities)\n",
    "    return itos.get(closest_idx, '<UNK>')\n",
    "\n",
    "def handle_unknown_words(words, model):\n",
    "    processed_words = []\n",
    "    word_embeddings = model.emb.weight.detach().cpu().numpy()\n",
    "\n",
    "    for word in words:\n",
    "        if word in stoi:\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            random_vector = np.random.randn(word_embeddings.shape[1])\n",
    "            closest_word = find_closest_word(random_vector, model)\n",
    "            st.warning(f\"Unknown word '{word}' replaced with '{closest_word}'\")\n",
    "            processed_words.append(closest_word)\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "def generate_text(model, context, max_words, block_size):\n",
    "    model.eval()\n",
    "\n",
    "    # Handle unknown words in the context\n",
    "    context = handle_unknown_words(context, model)\n",
    "\n",
    "    # Convert context to indices and pad/truncate to fit block size\n",
    "    context_indices = [stoi.get(word, stoi['<PAD>']) for word in context]\n",
    "    context_indices = [0] * (block_size - len(context_indices)) + context_indices[-block_size:]\n",
    "\n",
    "    generated_text = []\n",
    "    for _ in range(max_words):\n",
    "        x = torch.tensor([context_indices], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).squeeze(0)\n",
    "        next_word_idx = torch.multinomial(probs, 1).item()\n",
    "        next_word = itos.get(next_word_idx, '<UNK>')\n",
    "\n",
    "        if next_word == '<PAD>':\n",
    "            break\n",
    "\n",
    "        generated_text.append(next_word)\n",
    "        context_indices = context_indices[1:] + [next_word_idx]\n",
    "\n",
    "    return ' '.join(generated_text) if generated_text else '<No valid output>'\n",
    "\n",
    "def plot_word_relations(model, num_words_per_category=10):\n",
    "    embeddings = model.emb.weight.detach().cpu().numpy()\n",
    "\n",
    "    categories = {'synonyms': [], 'antonyms': [], 'pronouns': [], 'names': [], 'unrelated': []}\n",
    "\n",
    "    for word, idx in stoi.items():\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if word in ['he', 'she', 'they', 'it']:\n",
    "            categories['pronouns'].append(word)\n",
    "        elif word.istitle():\n",
    "            categories['names'].append(word)\n",
    "        else:\n",
    "            for syn in synsets:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() in stoi and lemma.name() != word:\n",
    "                        categories['synonyms'].append((word, lemma.name()))\n",
    "                    if lemma.antonyms():\n",
    "                        antonym = lemma.antonyms()[0].name()\n",
    "                        if antonym in stoi:\n",
    "                            categories['antonyms'].append((word, antonym))\n",
    "\n",
    "    for category in categories:\n",
    "        categories[category] = categories[category][:num_words_per_category]\n",
    "\n",
    "    all_words, all_categories = [], []\n",
    "    for category, word_pairs in categories.items():\n",
    "        for word in word_pairs:\n",
    "            if isinstance(word, tuple):\n",
    "                all_words.extend(word)\n",
    "                all_categories.extend([category] * 2)\n",
    "            else:\n",
    "                all_words.append(word)\n",
    "                all_categories.append(category)\n",
    "\n",
    "    word_indices = [stoi[word] for word in all_words if word in stoi]\n",
    "    selected_embeddings = embeddings[word_indices]\n",
    "\n",
    "    _, unique_indices = np.unique(selected_embeddings, axis=0, return_index=True)\n",
    "    selected_embeddings = selected_embeddings[unique_indices]\n",
    "    all_words = [all_words[i] for i in unique_indices]\n",
    "    all_categories = [all_categories[i] for i in unique_indices]\n",
    "\n",
    "    selected_embeddings += np.random.normal(0, 1e-3, selected_embeddings.shape)\n",
    "    selected_embeddings /= np.linalg.norm(selected_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    n_samples = len(selected_embeddings)\n",
    "    perplexity = min(30, n_samples - 1)\n",
    "\n",
    "    if n_samples < 2:\n",
    "        st.error(\"Not enough unique points to generate a plot.\")\n",
    "        return\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000)\n",
    "    tsne_embeddings = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "    df = pd.DataFrame({'word': all_words, 'x': tsne_embeddings[:, 0], 'y': tsne_embeddings[:, 1], 'category': all_categories})\n",
    "\n",
    "    plt.figure(figsize=(min(20, n_samples // 5), min(12, n_samples // 10)))\n",
    "    sns.scatterplot(data=df, x='x', y='y', hue='category', style='category', s=100, palette='deep')\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        plt.text(row['x'] + 0.01, row['y'] + 0.01, row['word'], fontsize=9)\n",
    "\n",
    "    plt.title(\"Word Embeddings Visualization by Category\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    st.pyplot(plt)\n",
    "\n",
    "st.title(\"Next Word Prediction App\")\n",
    "emb_dim = st.sidebar.selectbox(\"Embedding Dimension\", [128, 256], index=1)\n",
    "hidden_size = st.sidebar.selectbox(\"Hidden Size\", [512, 1024], index=1)\n",
    "activation = st.sidebar.selectbox(\"Activation Function\", [\"ReLU\", \"Tanh\"])\n",
    "block_size = st.sidebar.slider(\"Block Size\", 3, 15, default_block_size)\n",
    "random_seed = st.sidebar.number_input(\"Random Seed\", min_value=0, value=default_random_seed)\n",
    "\n",
    "model = load_model(emb_dim, hidden_size, activation, block_size, random_seed)\n",
    "\n",
    "user_input = st.text_area(\"Enter some text to generate the next words:\")\n",
    "if user_input:\n",
    "    context = user_input.lower().split()\n",
    "    max_words = st.slider(\"Number of Words to Predict\", 1, 200, 20)\n",
    "    with st.spinner(\"Generating text...\"):\n",
    "        generated_text = generate_text(model, context, max_words, block_size)\n",
    "    st.write(f\"**Generated Text:** {generated_text}\")\n",
    "\n",
    "num_words_per_category = st.sidebar.slider(\"Words per Category\", min_value=10, max_value=100, value=50, step=10)\n",
    "\n",
    "if st.sidebar.button(\"Visualize Word Relations\"):\n",
    "    st.write(\"Generating visualization...\")\n",
    "    plot_word_relations(model, num_words_per_category)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"streamlit_app.py\", \"w\") as file:\n",
    "    file.write(streamlit_code)\n",
    "\n",
    "print(\"Streamlit app code saved to streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit_app.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
